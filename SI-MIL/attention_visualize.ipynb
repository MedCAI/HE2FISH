{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 细胞级特征注意力整合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import glob\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import openslide\n",
    "from openslide.deepzoom import DeepZoomGenerator\n",
    "\n",
    "class AttentionQuerier:\n",
    "    \"\"\"\n",
    "    一个工具类，用于根据patch路径查询其在不确定后缀的attention map中的数值。\n",
    "    \"\"\"\n",
    "    def __init__(self, svs_root_dir, attn_root_dir, tile_size=224, overlap=0):\n",
    "        \"\"\"\n",
    "        初始化 AttentionQuerier。\n",
    "        Args:\n",
    "            svs_root_dir (str): 存放 .svs 文件的根目录。\n",
    "            attn_root_dir (str): 存放 attention map 子文件夹的根目录。\n",
    "            tile_size (int): 切片时使用的tile大小。\n",
    "            overlap (int): 切片时使用的重叠大小。\n",
    "        \"\"\"\n",
    "        self.svs_root_dir = svs_root_dir\n",
    "        self.attn_root_dir = attn_root_dir\n",
    "        self.tile_size = tile_size\n",
    "        self.overlap = overlap\n",
    "        \n",
    "        # 预检查路径是否存在\n",
    "        if not os.path.isdir(svs_root_dir):\n",
    "            raise FileNotFoundError(f\"SVS root directory not found: {svs_root_dir}\")\n",
    "        if not os.path.isdir(attn_root_dir):\n",
    "            raise FileNotFoundError(f\"Attention map root directory not found: {attn_root_dir}\")\n",
    "\n",
    "    def _find_attention_maps(self, svs_basename):\n",
    "        \"\"\"根据SVS基础文件名，搜索所有可能的attention map路径。\"\"\"\n",
    "        search_pattern = os.path.join(self.attn_root_dir, f\"{svs_basename}_*/attn.png\")\n",
    "        found_paths = glob.glob(search_pattern)\n",
    "        \n",
    "        results = {}\n",
    "        for path in found_paths:\n",
    "            # 从路径中提取后缀, e.g., 'BCL2'\n",
    "            # 路径: .../21-11167_BCL2/attn.png -> 文件夹: 21-11167_BCL2\n",
    "            folder_name = os.path.basename(os.path.dirname(path))\n",
    "            # 从文件夹名中提取后缀\n",
    "            suffix = folder_name.replace(f\"{svs_basename}_\", \"\")\n",
    "            results[suffix] = path\n",
    "            \n",
    "        return results\n",
    "\n",
    "    def query_patch_attention(self, patch_path, method='mean'):\n",
    "        \"\"\"\n",
    "        主函数：为单个patch查询其在所有可用attention map中的数值。\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # 1. 解析Patch路径，获取SVS基础文件名\n",
    "            svs_basename = os.path.basename(os.path.dirname(patch_path))\n",
    "            \n",
    "            # 2. 搜索所有相关的Attention Map\n",
    "            found_maps = self._find_attention_maps(svs_basename)\n",
    "            \n",
    "            if not found_maps:\n",
    "                print(f\"警告: 未找到与 '{svs_basename}' 相关的attention map。\")\n",
    "                return {}\n",
    "\n",
    "            # 3. 获取WSI信息 (只需要获取一次)\n",
    "            svs_path = os.path.join(self.svs_root_dir, f\"{svs_basename}.svs\")\n",
    "            if not os.path.exists(svs_path):\n",
    "                raise FileNotFoundError(f\"SVS file not found at: {svs_path}\")\n",
    "            \n",
    "            slide = openslide.open_slide(svs_path)\n",
    "            dz = DeepZoomGenerator(slide, self.tile_size, self.overlap)\n",
    "            wsi_dims = slide.dimensions\n",
    "            highest_zoom_level = dz.level_count - 1\n",
    "\n",
    "            # 4. 解码Patch文件名，获取WSI坐标\n",
    "            patch_filename_only = os.path.basename(patch_path)\n",
    "            parts = os.path.splitext(patch_filename_only)[0].split('_')\n",
    "            level, col, row = int(parts[0]), int(parts[1]), int(parts[2])\n",
    "            \n",
    "            downsample_factor = 2 ** (highest_zoom_level - level)\n",
    "            x_start_wsi = col * self.tile_size * downsample_factor\n",
    "            y_start_wsi = row * self.tile_size * downsample_factor\n",
    "            x_end_wsi = x_start_wsi + (self.tile_size * downsample_factor)\n",
    "            y_end_wsi = y_start_wsi + (self.tile_size * downsample_factor)\n",
    "\n",
    "            # 5. 循环处理每个找到的attention map\n",
    "            final_scores = {}\n",
    "            for suffix, attn_path in found_maps.items():\n",
    "                print(f\"-> 正在从 '{suffix}' 注意力图获取数值...\")\n",
    "                \n",
    "                # 加载attention map为灰度图\n",
    "                attn_map_img = Image.open(attn_path)\n",
    "                attn_map_array = np.array(attn_map_img, dtype=np.float32)\n",
    "                attn_map_array = attn_map_array / 255.0\n",
    "\n",
    "                # 计算缩放比例\n",
    "                attn_height, attn_width = attn_map_array.shape\n",
    "                scale_factor_width = wsi_dims[0] / attn_width\n",
    "                scale_factor_height = wsi_dims[1] / attn_height\n",
    "\n",
    "                # 映射坐标\n",
    "                x_start_attn = int(x_start_wsi / scale_factor_width)\n",
    "                y_start_attn = int(y_start_wsi / scale_factor_height)\n",
    "                x_end_attn = int(x_end_wsi / scale_factor_width)\n",
    "                y_end_attn = int(y_end_wsi / scale_factor_height)\n",
    "\n",
    "                # 提取区域并计算分数\n",
    "                attention_region = attn_map_array[y_start_attn:y_end_attn, x_start_attn:x_end_attn]\n",
    "                \n",
    "                if attention_region.size == 0:\n",
    "                    score = 0.0\n",
    "                elif method == 'mean':\n",
    "                    score = np.mean(attention_region)\n",
    "                elif method == 'max':\n",
    "                    score = np.max(attention_region)\n",
    "                else:\n",
    "                    raise ValueError(\"Method must be 'mean' or 'max'\")\n",
    "                \n",
    "                final_scores[suffix] = score\n",
    "                \n",
    "            return final_scores\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"处理 {patch_path} 时发生错误: {e}\")\n",
    "            return None\n",
    "\n",
    "# 防止因图像过大而报错\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "\n",
    "class AttentionQuerier:\n",
    "    \"\"\"\n",
    "    一个工具类，用于根据patch路径查询其在不确定后缀的attention map中的数值。\n",
    "    \"\"\"\n",
    "    def __init__(self, svs_root_dir, attn_root_dir, tile_size=224, overlap=0):\n",
    "        self.svs_root_dir = svs_root_dir\n",
    "        self.attn_root_dir = attn_root_dir\n",
    "        self.tile_size = tile_size\n",
    "        self.overlap = overlap\n",
    "        \n",
    "        if not os.path.isdir(svs_root_dir):\n",
    "            raise FileNotFoundError(f\"SVS root directory not found: {svs_root_dir}\")\n",
    "        if not os.path.isdir(attn_root_dir):\n",
    "            raise FileNotFoundError(f\"Attention map root directory not found: {attn_root_dir}\")\n",
    "\n",
    "    def _find_attention_maps(self, svs_basename):\n",
    "        search_pattern = os.path.join(self.attn_root_dir, f\"{svs_basename}_*/attn.png\")\n",
    "        found_paths = glob.glob(search_pattern)\n",
    "        results = {}\n",
    "        for path in found_paths:\n",
    "            folder_name = os.path.basename(os.path.dirname(path))\n",
    "            suffix = folder_name.replace(f\"{svs_basename}_\", \"\")\n",
    "            results[suffix] = path\n",
    "        return results\n",
    "\n",
    "    def query_patch_attention(self, patch_path, method='mean'):\n",
    "        try:\n",
    "            svs_basename = os.path.basename(os.path.dirname(patch_path))\n",
    "            found_maps = self._find_attention_maps(svs_basename)\n",
    "            \n",
    "            if not found_maps:\n",
    "                return {}\n",
    "\n",
    "            svs_path = os.path.join(self.svs_root_dir, f\"{svs_basename}.svs\")\n",
    "            if not os.path.exists(svs_path):\n",
    "                # 如果找不到SVS，也无法继续\n",
    "                print(f\"警告: 找不到SVS文件 {svs_path}，跳过patch {patch_path}\")\n",
    "                return {}\n",
    "            \n",
    "            slide = openslide.open_slide(svs_path)\n",
    "            dz = DeepZoomGenerator(slide, self.tile_size, self.overlap)\n",
    "            wsi_dims = slide.dimensions\n",
    "            highest_zoom_level = dz.level_count - 1\n",
    "\n",
    "            patch_filename_only = os.path.basename(patch_path)\n",
    "            parts = os.path.splitext(patch_filename_only)[0].split('_')\n",
    "            level, col, row = int(parts[0]), int(parts[1]), int(parts[2])\n",
    "            \n",
    "            downsample_factor = 2 ** (highest_zoom_level - level)\n",
    "            x_start_wsi = col * self.tile_size * downsample_factor\n",
    "            y_start_wsi = row * self.tile_size * downsample_factor\n",
    "            x_end_wsi = x_start_wsi + (self.tile_size * downsample_factor)\n",
    "            y_end_wsi = y_start_wsi + (self.tile_size * downsample_factor)\n",
    "\n",
    "            final_scores = {}\n",
    "            for suffix, attn_path in found_maps.items():\n",
    "                attn_map_img = Image.open(attn_path)\n",
    "                attn_map_array = np.array(attn_map_img, dtype=np.float32) / 255.0\n",
    "\n",
    "                attn_height, attn_width = attn_map_array.shape\n",
    "                scale_factor_width = wsi_dims[0] / attn_width\n",
    "                scale_factor_height = wsi_dims[1] / attn_height\n",
    "\n",
    "                x_start_attn = int(x_start_wsi / scale_factor_width)\n",
    "                y_start_attn = int(y_start_wsi / scale_factor_height)\n",
    "                x_end_attn = int(x_end_wsi / scale_factor_width)\n",
    "                y_end_attn = int(y_end_wsi / scale_factor_height)\n",
    "\n",
    "                attention_region = attn_map_array[y_start_attn:y_end_attn, x_start_attn:x_end_attn]\n",
    "                \n",
    "                if attention_region.size == 0:\n",
    "                    score = np.nan # 使用NaN表示无效值\n",
    "                elif method == 'mean':\n",
    "                    score = np.mean(attention_region)\n",
    "                elif method == 'max':\n",
    "                    score = np.max(attention_region)\n",
    "                else:\n",
    "                    raise ValueError(\"Method must be 'mean' or 'max'\")\n",
    "                \n",
    "                final_scores[suffix] = score\n",
    "                \n",
    "            return final_scores\n",
    "        except Exception as e:\n",
    "            print(f\"处理 {patch_path} 时发生严重错误，将返回空结果: {e}\")\n",
    "            return {}\n",
    "\n",
    "\n",
    "def process_csv_with_attention(input_csv, output_csv, patch_column_name, **kwargs):\n",
    "    \"\"\"\n",
    "    读取CSV文件，为每一行计算attention分数，并将分数作为新列添加到文件末尾。\n",
    "\n",
    "    Args:\n",
    "        input_csv (str): 输入的CSV文件路径。\n",
    "        output_csv (str): 输出的CSV文件路径。\n",
    "        patch_column_name (str): 包含patch路径的列的名称。\n",
    "        **kwargs: 传递给 AttentionQuerier 的参数 (svs_root_dir, attn_root_dir, ...)。\n",
    "    \"\"\"\n",
    "    # 1. 读取输入的CSV文件\n",
    "    print(f\"正在读取输入文件: {input_csv}\")\n",
    "    try:\n",
    "        df = pd.read_csv(input_csv)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"错误: 输入文件未找到 -> {input_csv}\")\n",
    "        return\n",
    "\n",
    "    if patch_column_name not in df.columns:\n",
    "        print(f\"错误: 在CSV文件中找不到指定的列 '{patch_column_name}'。\")\n",
    "        print(f\"可用的列有: {list(df.columns)}\")\n",
    "        return\n",
    "\n",
    "    # 2. 初始化 AttentionQuerier\n",
    "    print(\"正在初始化Attention Querier...\")\n",
    "    querier = AttentionQuerier(**kwargs)\n",
    "\n",
    "    # 3. 循环处理每一行，获取attention分数\n",
    "    results_list = []\n",
    "    patch_paths = df[patch_column_name]\n",
    "    \n",
    "    print(f\"开始处理 {len(df)} 个patch...\")\n",
    "    for patch_path in tqdm(patch_paths, desc=\"查询Attention分数\"):\n",
    "        scores = querier.query_patch_attention(patch_path, method='mean')\n",
    "        results_list.append(scores)\n",
    "\n",
    "    # 4. 将结果列表转换为Pandas DataFrame\n",
    "    # Pandas 会自动处理列的创建和缺失值(NaN)的填充\n",
    "    print(\"正在整合注意力分数...\")\n",
    "    df_attention = pd.DataFrame(results_list)\n",
    "\n",
    "    # 5. 为新列添加前缀，使其更具可读性\n",
    "    df_attention = df_attention.add_prefix('attention_')\n",
    "\n",
    "    # 6. 将原始DataFrame与新的attention分数DataFrame合并\n",
    "    print(\"正在合并原始数据和新分数...\")\n",
    "    # 使用 .join() 可以避免因索引问题导致的错位\n",
    "    df_final = df.join(df_attention)\n",
    "\n",
    "    # 7. 保存到新的CSV文件\n",
    "    print(f\"正在将最终结果保存到: {output_csv}\")\n",
    "    df_final.to_csv(output_csv, index=False)\n",
    "    \n",
    "    print(\"\\n处理完成！\")\n",
    "    print(f\"结果已保存在: {output_csv}\")\n",
    "    print(\"\\n新文件的前5行预览:\")\n",
    "    print(df_final.head())\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    SVS_ROOT_DIR = '/data/ceiling/data/DLBCL/WSI/TCH'\n",
    "    ATTN_ROOT_DIR = '/data/ceiling/workspace/DLBLC/visualization/AMIL_MOE_Sampling/TCH'\n",
    "    \n",
    "    INPUT_CSV_PATH = '/data/cjy/nuclei/datasets/DLBCL/TCH/features/cell_athena_sna.csv' \n",
    "    OUTPUT_CSV_PATH = '/data/cjy/nuclei/datasets/DLBCL/TCH/features/cell_athena_sna_attn.csv'\n",
    "\n",
    "    PATCH_COLUMN_NAME = 'Unnamed: 0' \n",
    "\n",
    "    process_csv_with_attention(\n",
    "        input_csv=INPUT_CSV_PATH,\n",
    "        output_csv=OUTPUT_CSV_PATH,\n",
    "        patch_column_name=PATCH_COLUMN_NAME,\n",
    "        svs_root_dir=SVS_ROOT_DIR,\n",
    "        attn_root_dir=ATTN_ROOT_DIR,\n",
    "        tile_size=224\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特征与Target 相关性分析\n",
    "\n",
    "## 相关性文件生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def run_correlation_analysis(csv_path, attention_columns, top_n=20, output_dir='correlation_reports'):\n",
    "    print(f\"步骤 1: 正在加载数据从 '{csv_path}'...\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        print(f\"已创建输出目录: '{output_dir}'\")\n",
    "        \n",
    "    feature_columns = df.select_dtypes(include=np.number).columns.tolist()\n",
    "    for attn_col in attention_columns:\n",
    "        if attn_col in feature_columns:\n",
    "            feature_columns.remove(attn_col)\n",
    "            \n",
    "    print(f\"自动检测到 {len(feature_columns)} 个数值型特征列进行分析。\")\n",
    "    \n",
    "    for attn_col in attention_columns:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"相关性分析报告: {attn_col.upper()}\")\n",
    "        print(\"=\"*80)\n",
    "    \n",
    "        print(f\"正在计算 '{attn_col}' 与所有特征的相关性...\")\n",
    "        \n",
    "        correlations = df[feature_columns].corrwith(df[attn_col])\n",
    "        correlations = correlations.dropna().sort_values(ascending=False)\n",
    "        \n",
    "        # print(f\"\\n[+] 与 '{attn_col}' 【正相关性最强】的Top {top_n}个特征:\")\n",
    "        # print(\"    (这些特征值越高，Attention值也倾向于越高)\")\n",
    "        # print(correlations.head(top_n).to_string())\n",
    "        \n",
    "        # print(f\"\\n[-] 与 '{attn_col}' 【负相关性最强】的Top {top_n}个特征:\")\n",
    "        # print(\"    (这些特征值越高，Attention值反而倾向于越低)\")\n",
    "\n",
    "        # 对于负相关，我们需要对升序排列的结果取头部\n",
    "        print(correlations.sort_values(ascending=True).head(top_n).to_string())\n",
    "        \n",
    "        output_filename = os.path.join(output_dir, f'correlation_report_{attn_col}.csv')\n",
    "        correlations.to_frame(name='pearson_correlation').to_csv(output_filename)\n",
    "        print(f\"\\n>>> 完整的相关性分析报告已保存到: '{output_filename}'\")\n",
    "\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"所有分析任务完成！\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    INPUT_CSV_WITH_ATTN = '/data/cjy/nuclei/datasets/DLBCL/TCH/features/cell_athena_sna_attn.csv'\n",
    "    ATTENTION_COLUMNS_TO_ANALYZE = ['attention_BCL2', 'attention_BCL6', 'attention_MYC']\n",
    "    OUTPUT_DIRECTORY = 'attention_report'\n",
    "\n",
    "    run_correlation_analysis(\n",
    "        csv_path=INPUT_CSV_WITH_ATTN,\n",
    "        attention_columns=ATTENTION_COLUMNS_TO_ANALYZE,\n",
    "        output_dir=OUTPUT_DIRECTORY,\n",
    "        top_n=20 # 显示前20个结果\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TOP20 皮尔森相关系数绘图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# 设置全局字体和字号\n",
    "plt.rcParams[\"font.family\"] = \"Arial\"\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False\n",
    "plt.rcParams.update({\n",
    "    'font.size': 14,          # 基础字号\n",
    "    'axes.labelsize': 16,     # 坐标轴标题\n",
    "    'xtick.labelsize': 14,    # 横轴刻度\n",
    "    'ytick.labelsize': 14,    # 纵轴刻度\n",
    "    'legend.fontsize': 14,    # 图例\n",
    "    'axes.titlesize': 18      # 标题字号\n",
    "})\n",
    "\n",
    "def visualize_focused_correlation_heatmap(report_dir, attention_sources, top_n=20):\n",
    "    print(\"步骤 1: 正在加载并整合所有分析报告...\")\n",
    "    all_corrs = []\n",
    "    \n",
    "    for source in attention_sources:\n",
    "        file_path = os.path.join(report_dir, f'correlation_report_{source}.csv')\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, index_col=0)\n",
    "            df.columns = [source]\n",
    "            all_corrs.append(df)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"警告: 未找到报告文件 '{file_path}'，已跳过。\")\n",
    "            continue\n",
    "\n",
    "    if not all_corrs:\n",
    "        print(\"错误: 未加载任何有效的报告文件，无法继续。\")\n",
    "        return\n",
    "\n",
    "    corr_df = pd.concat(all_corrs, axis=1)\n",
    "    print(f\"成功加载并整合了 {len(corr_df)} 个特征的相关性数据。\")\n",
    "\n",
    "    print(\"\\n步骤 2: 正在根据关键词筛选'细胞级'特征...\")\n",
    "    CELLULAR_FEATURES_KEYWORDS = [\n",
    "        'number of', 'mean of their', 'std of their',\n",
    "        'skew of their', 'kurtosis of their', 'Infiltration of'\n",
    "    ]\n",
    "    focused_feature_index = [\n",
    "        feature_name for feature_name in corr_df.index\n",
    "        if any(keyword in feature_name for keyword in CELLULAR_FEATURES_KEYWORDS)\n",
    "    ]\n",
    "    corr_df_focused = corr_df.loc[focused_feature_index]\n",
    "    \n",
    "    print(f\"已筛选出 {len(corr_df_focused)} 个细胞级特征进行下一步分析。\")\n",
    "    if len(corr_df_focused) == 0:\n",
    "        print(\"错误: 未找到任何匹配的细胞级特征，请检查关键词或报告中的列名。\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\n步骤 3: 正在从已筛选的特征中，找出Top {top_n}个最重要的...\")\n",
    "    corr_df_focused['importance_score'] = corr_df_focused.abs().mean(axis=1)\n",
    "    top_features_df = corr_df_focused.sort_values(by='importance_score', ascending=False).head(top_n)\n",
    "    plot_data = top_features_df.drop(columns=['importance_score'])\n",
    "\n",
    "    simplified_columns = {col: col.replace('attention_', '') for col in plot_data.columns}\n",
    "    plot_data = plot_data.rename(columns=simplified_columns)\n",
    "\n",
    "    print(\"\\n步骤 4: 正在生成聚焦于细胞级指标的热力图...\")\n",
    "    plt.figure(figsize=(10, 12))\n",
    "    sns.heatmap(\n",
    "        plot_data, \n",
    "        annot=True, \n",
    "        cmap='coolwarm', \n",
    "        vmin=-1, vmax=1, center=0,\n",
    "        fmt='.2f',\n",
    "        linewidths=.5,\n",
    "        cbar_kws={'label': 'Pearson Correlation Coefficient'}\n",
    "    )\n",
    "    \n",
    "    plt.title(f'Top {top_n} Cellular Features Correlated with Attention', pad=10, weight='bold')\n",
    "    plt.ylabel('Direct Cellular & Interaction Features')\n",
    "    plt.xlabel('Attention Source')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('/data114_4/chenjy/DLBCL/model/SI-MIL/Top_Cellular_Features_Correlation.pdf', dpi=300)\n",
    "    print(\"\\n步骤 5: 图表已保存并显示。\")\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    REPORT_DIRECTORY = 'attention_report'\n",
    "    ATTENTION_SOURCES_TO_COMPARE = ['attention_BCL2', 'attention_BCL6', 'attention_MYC']\n",
    "    \n",
    "    visualize_focused_correlation_heatmap(\n",
    "        report_dir=REPORT_DIRECTORY,\n",
    "        attention_sources=ATTENTION_SOURCES_TO_COMPARE,\n",
    "        top_n=20\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 不同Target下 细胞特征重要性分析\n",
    "## 相关文件生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def run_and_save_individual_analysis_with_cohens_d(csv_path, attention_columns, quantile_threshold=0.9, top_n=20, output_dir='analysis_reports_cohens_d'):\n",
    "    \"\"\"\n",
    "    对一个CSV文件中的多个Attention列进行批量分组对比分析。\n",
    "    \"\"\"\n",
    "    # --- 1. 加载数据并设置输出目录 ---\n",
    "    print(f\"步骤 1: 正在加载数据从 '{csv_path}'...\")\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"错误: 文件未找到 -> {csv_path}\")\n",
    "        return\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        print(f\"已创建输出目录: '{output_dir}'\")\n",
    "        \n",
    "    all_feature_columns = [col for col in df.columns if col not in attention_columns]\n",
    "    \n",
    "    # --- 2. 循环处理每一个Attention列 ---\n",
    "    for attn_col in attention_columns:\n",
    "        if attn_col not in df.columns:\n",
    "            print(f\"\\n警告: 在CSV中找不到列 '{attn_col}'，已跳过。\")\n",
    "            continue\n",
    "\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"               分析对象: {attn_col.upper()}\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        df_cleaned = df[[attn_col] + all_feature_columns].dropna()\n",
    "        \n",
    "        # --- 分组 ---\n",
    "        threshold_value = df_cleaned[attn_col].quantile(quantile_threshold)\n",
    "        high_attention_mask = df_cleaned[attn_col] >= threshold_value\n",
    "        \n",
    "        high_group_df = df_cleaned[high_attention_mask]\n",
    "        baseline_group_df = df_cleaned[~high_attention_mask]\n",
    "        \n",
    "        n_high, n_base = len(high_group_df), len(baseline_group_df)\n",
    "        print(f\"定义 '高分组' ({n_high}个样本) vs. '基线组' ({n_base}个样本)。\")\n",
    "\n",
    "        if n_high < 2 or n_base < 2:\n",
    "            print(\"警告: 某个分组的样本太少，无法进行有效分析，已跳过。\")\n",
    "            continue\n",
    "\n",
    "        # --- 计算统计量 ---\n",
    "        mean_high = high_group_df[all_feature_columns].mean(numeric_only=True)\n",
    "        mean_base = baseline_group_df[all_feature_columns].mean(numeric_only=True)\n",
    "        std_high = high_group_df[all_feature_columns].std(numeric_only=True)\n",
    "        std_base = baseline_group_df[all_feature_columns].std(numeric_only=True)\n",
    "\n",
    "        # 计算合并标准差 (Pooled Standard Deviation)\n",
    "        pooled_std = np.sqrt( ((n_high - 1) * std_high**2 + (n_base - 1) * std_base**2) / (n_high + n_base - 2) )\n",
    "        \n",
    "        # 计算科恩d值\n",
    "        cohens_d = (mean_high - mean_base) / (pooled_std + 1e-8) # 加一个极小值避免除以零\n",
    "\n",
    "        # --- 创建结果DataFrame并排序 ---\n",
    "        result_df = pd.DataFrame({\n",
    "            'cohens_d': cohens_d,\n",
    "            'mean_in_high_group': mean_high,\n",
    "            'mean_in_baseline_group': mean_base,\n",
    "        })\n",
    "        \n",
    "        result_df['abs_cohens_d'] = abs(result_df['cohens_d'])\n",
    "        result_df = result_df.sort_values(by='abs_cohens_d', ascending=False)\n",
    "        \n",
    "        # --- 打印和保存 ---\n",
    "        print(f\"\\n[!] 与 '{attn_col}' 相关的【效应量最大】的Top {top_n}个特征 (基于科恩d值):\")\n",
    "        print(result_df.drop(columns=['abs_cohens_d']).head(top_n).to_string())\n",
    "        \n",
    "        output_filename = os.path.join(output_dir, f'analysis_report_{attn_col}.csv')\n",
    "        result_df.to_csv(output_filename)\n",
    "        print(f\"\\n>>> 详细分析报告已成功保存到: '{output_filename}'\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"所有分析任务完成！\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    INPUT_CSV_WITH_ATTN = '/data/cjy/nuclei/datasets/DLBCL/TCH/features/cell_athena_sna_attn.csv'\n",
    "    ATTENTION_COLUMNS_TO_ANALYZE = ['attention_BCL2', 'attention_BCL6', 'attention_MYC']\n",
    "    OUTPUT_DIRECTORY = '/data/cjy/nuclei/models/SI-MIL/attention_report_cohens_d/'\n",
    "\n",
    "    run_and_save_individual_analysis_with_cohens_d(\n",
    "        csv_path=INPUT_CSV_WITH_ATTN,\n",
    "        attention_columns=ATTENTION_COLUMNS_TO_ANALYZE,\n",
    "        output_dir=OUTPUT_DIRECTORY,\n",
    "        quantile_threshold=0.9,\n",
    "        top_n=20\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TOP15 重要特征绘图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# --- 全局设置字体和字号 ---\n",
    "try:\n",
    "    plt.rcParams['font.family'] = 'Arial'\n",
    "    plt.rcParams['axes.unicode_minus'] = False \n",
    "    print(\"✅ 全局字体已成功设置为 Arial。\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ 设置全局字体为 Arial 失败。错误: {e}\")\n",
    "    plt.rcParams['font.family'] = 'sans-serif'\n",
    "\n",
    "# ---- 全局字号 ----\n",
    "plt.rcParams.update({\n",
    "    'font.size': 14,          # 基础字号\n",
    "    'axes.labelsize': 16,     # 坐标轴标题\n",
    "    'xtick.labelsize': 14,    # 横轴刻度\n",
    "    'ytick.labelsize': 14,    # 纵轴刻度\n",
    "    'legend.fontsize': 14     # 图例\n",
    "})\n",
    "\n",
    "\n",
    "def show_focused_cohens_d_plot(report_dir, attention_sources, top_n=20, color_map=None):\n",
    "    \"\"\"\n",
    "    读取基于科恩d值的分析报告，筛选细胞级特征，并创建分组柱状图。\n",
    "    此版本使用全局字体与字号设置，代码更简洁。\n",
    "    \"\"\"\n",
    "    # --- 1. 加载并整合所有报告数据 ---\n",
    "    print(\"步骤 1: 正在加载基于科恩d值的分析报告...\")\n",
    "    all_metrics = []\n",
    "    \n",
    "    for source in attention_sources:\n",
    "        file_path = os.path.join(report_dir, f'analysis_report_{source}.csv')\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, index_col=0)\n",
    "            all_metrics.append(df[['cohens_d']].rename(columns={'cohens_d': source}))\n",
    "        except FileNotFoundError:\n",
    "            print(f\"警告: 未找到报告文件 '{file_path}'，已跳过。\")\n",
    "            continue\n",
    "            \n",
    "    if not all_metrics:\n",
    "        print(\"错误: 未加载任何有效的报告文件，无法继续。\")\n",
    "        return\n",
    "\n",
    "    metric_df = pd.concat(all_metrics, axis=1)\n",
    "\n",
    "    # --- 2. 筛选细胞级特征 ---\n",
    "    print(\"\\n步骤 2: 正在筛选'细胞级'特征...\")\n",
    "    CELLULAR_FEATURES_KEYWORDS = [\n",
    "        'number of', 'mean of their', 'std of their', \n",
    "        'skew of their', 'kurtosis of their', 'Infiltration of'\n",
    "    ]\n",
    "    focused_feature_index = [\n",
    "        name for name in metric_df.index if any(kw in name for kw in CELLULAR_FEATURES_KEYWORDS)\n",
    "    ]\n",
    "    metric_df_focused = metric_df.loc[focused_feature_index]\n",
    "    \n",
    "    # --- 3. 找出最重要的Top-N ---\n",
    "    print(f\"\\n步骤 3: 正在从已筛选的特征中，找出Top {top_n}个最重要的...\")\n",
    "    metric_df_focused['importance_score'] = metric_df_focused.abs().mean(axis=1)\n",
    "    top_features_df = metric_df_focused.sort_values(by='importance_score', ascending=False).head(top_n)\n",
    "    plot_data = top_features_df.drop(columns=['importance_score']).iloc[::-1]\n",
    "    \n",
    "    simplified_columns = {col: col.replace('attention_', '') for col in plot_data.columns}\n",
    "    plot_data = plot_data.rename(columns=simplified_columns)\n",
    "\n",
    "    # --- 4. 创建分组柱状图 ---\n",
    "    print(\"\\n步骤 4: 正在生成可视化图表...\")\n",
    "    n_sources = len(plot_data.columns)\n",
    "    bar_width = 0.8 / n_sources\n",
    "    index = np.arange(len(plot_data))\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 9))\n",
    "\n",
    "    for i, source_name in enumerate(plot_data.columns):\n",
    "        bar_positions = index + (i - (n_sources - 1) / 2) * bar_width\n",
    "        values = plot_data[source_name]\n",
    "        bar_color = color_map.get(source_name) if color_map else None\n",
    "        ax.barh(bar_positions, values, height=bar_width, label=source_name, alpha=0.85, color=bar_color)\n",
    "\n",
    "    # --- 5. 美化图表 ---\n",
    "    ax.set_xlabel(\"Standardized Mean Difference (Cohen's d)\")\n",
    "    ax.set_ylabel('Direct Cellular & Interaction Features')\n",
    "    ax.set_title(f'Top {top_n} Cellular Features with Highest Effect Size', pad=10, weight='bold')\n",
    "\n",
    "    ax.set_yticks(index)\n",
    "    ax.set_yticklabels(plot_data.index)\n",
    "    \n",
    "    ax.axvline(0, color='black', linewidth=1.0, linestyle='-')\n",
    "    ax.grid(axis='x', linestyle=':', alpha=0.7)\n",
    "    ax.legend(title=\"Attention Source\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # --- 6. 保存图表 ---\n",
    "    output_path = '/data114_4/chenjy/DLBCL/model/SI-MIL/Top_Cellular_Features.pdf'\n",
    "    plt.savefig(output_path, dpi=300)\n",
    "    print(f\"\\n步骤 5: 图表已保存至 '{output_path}'\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "#                                使用示例\n",
    "# ==============================================================================\n",
    "if __name__ == '__main__':\n",
    "    REPORT_DIRECTORY = '/data114_4/chenjy/DLBCL/model/SI-MIL/attention_report_cohens_d' \n",
    "    ATTENTION_SOURCES_TO_COMPARE = ['attention_BCL2', 'attention_BCL6', 'attention_MYC']\n",
    "    \n",
    "    COLOR_MAPPING = {\n",
    "        'BCL2': '#78C8BB',\n",
    "        'MYC':  '#F4E07D',\n",
    "        'BCL6': '#EB9797'\n",
    "    }\n",
    "    \n",
    "    show_focused_cohens_d_plot(\n",
    "        report_dir=REPORT_DIRECTORY,\n",
    "        attention_sources=ATTENTION_SOURCES_TO_COMPARE,\n",
    "        top_n=15,\n",
    "        color_map=COLOR_MAPPING\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
